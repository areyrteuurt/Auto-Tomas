import asyncio
import aiohttp
import json
import re
import logging
from bs4 import BeautifulSoup
import os
import shutil
from datetime import datetime
import pytz
import base64
from urllib.parse import parse_qs, unquote

# è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•çš„ç»å¯¹è·¯å¾„
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.abspath(os.path.join(SCRIPT_DIR, '..'))

# ä½¿ç”¨ç»å¯¹è·¯å¾„å¼•ç”¨æ–‡ä»¶
URLS_FILE = os.path.join(SCRIPT_DIR, 'urls.txt')
KEYWORDS_FILE = os.path.join(SCRIPT_DIR, 'key.json')
PROTOCOL_OUTPUT_DIR = os.path.join(PROJECT_ROOT, 'configs/protocols')
COUNTRY_OUTPUT_DIR = os.path.join(PROJECT_ROOT, 'configs/countries')
README_FILE = os.path.join(PROJECT_ROOT, 'README.md')
REQUEST_TIMEOUT = 15
CONCURRENT_REQUESTS = 10
MAX_CONFIG_LENGTH = 1500
MIN_PERCENT25_COUNT = 15

logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')

PROTOCOL_CATEGORIES = [
    "Vmess", "Vless", "Trojan", "ShadowSocks", "ShadowSocksR",
    "Tuic", "Hysteria2", "WireGuard"
]

def is_persian_like(text):
    if not isinstance(text, str) or not text.strip():
        return False
    has_persian_char = False
    has_latin_char = False
    for char in text:
        if '\u0600' <= char <= '\u06FF' or char in ['\u200C', '\u200D']:
            has_persian_char = True
        elif 'a' <= char.lower() <= 'z':
            has_latin_char = True
    return has_persian_char and not has_latin_char

def decode_base64(data):
    try:
        data = data.replace('_', '/').replace('-', '+')
        missing_padding = len(data) % 4
        if missing_padding:
            data += '=' * (4 - missing_padding)
        return base64.b64decode(data).decode('utf-8')
    except Exception:
        return None

def get_vmess_name(vmess_link):
    if not vmess_link.startswith("vmess://"):
        return None
    try:
        b64_part = vmess_link[8:]
        decoded_str = decode_base64(b64_part)
        if decoded_str:
            vmess_json = json.loads(decoded_str)
            return vmess_json.get('ps')
    except Exception as e:
        logging.warning(f"Failed to parse Vmess name from {vmess_link[:30]}...: {e}")
    return None

def get_ssr_name(ssr_link):
    if not ssr_link.startswith("ssr://"):
        return None
    try:
        b64_part = ssr_link[6:]
        decoded_str = decode_base64(b64_part)
        if not decoded_str:
            return None
        parts = decoded_str.split('/?')
        if len(parts) < 2:
            return None
        params_str = parts[1]
        params = parse_qs(params_str)
        if 'remarks' in params and params['remarks']:
            remarks_b64 = params['remarks'][0]
            return decode_base64(remarks_b64)
    except Exception as e:
        logging.warning(f"Failed to parse SSR name from {ssr_link[:30]}...: {e}")
    return None

def get_vless_name(vless_link):
    if not vless_link.startswith("vless://"):
        return None
    try:
        # vless://uuid@host:port?encryption=none&security=tls&sni=example.com#name
        if '#' in vless_link:
            name_part = vless_link.split('#', 1)[1]
            return unquote(name_part).strip()
    except Exception as e:
        logging.warning(f"Failed to parse Vless name from {vless_link[:30]}...: {e}")
    return None

def get_trojan_name(trojan_link):
    if not trojan_link.startswith("trojan://"):
        return None
    try:
        # trojan://password@host:port#name
        if '#' in trojan_link:
            name_part = trojan_link.split('#', 1)[1]
            return unquote(name_part).strip()
    except Exception as e:
        logging.warning(f"Failed to parse Trojan name from {trojan_link[:30]}...: {e}")
    return None

def get_shadowsocks_name(ss_link):
    if not ss_link.startswith("ss://"):
        return None
    try:
        # ss://base64_encoded#name
        if '#' in ss_link:
            name_part = ss_link.split('#', 1)[1]
            return unquote(name_part).strip()
    except Exception as e:
        logging.warning(f"Failed to parse Shadowsocks name from {ss_link[:30]}...: {e}")
    return None

def get_hysteria2_name(hy2_link):
    if not hy2_link.startswith("hy2://"):
        return None
    try:
        # hy2://password@host:port#name
        if '#' in hy2_link:
            name_part = hy2_link.split('#', 1)[1]
            return unquote(name_part).strip()
    except Exception as e:
        logging.warning(f"Failed to parse Hysteria2 name from {hy2_link[:30]}...: {e}")
    return None

def get_wireguard_name(wg_link):
    if not (wg_link.startswith("wg://") or 'WireGuard' in wg_link):
        return None
    try:
        # WireGuardé…ç½®é€šå¸¸åŒ…å«#name
        if '#' in wg_link:
            name_part = wg_link.split('#', 1)[1]
            return unquote(name_part).strip()
    except Exception as e:
        logging.warning(f"Failed to parse WireGuard name from {wg_link[:30]}...: {e}")
    return None

def should_filter_config(config):
    # è¿‡æ»¤åŒ…å«å¹¿å‘Šæˆ–å¯ç–‘å†…å®¹çš„é…ç½®
    if 'i_love_' in config.lower():
        return True
    # è¿‡æ»¤URLç¼–ç å¼‚å¸¸ï¼ˆ%25è¿‡å¤šï¼‰çš„é…ç½®
    percent25_count = config.count('%25')
    if percent25_count >= MIN_PERCENT25_COUNT:
        return True
    # è¿‡æ»¤è¿‡é•¿çš„é…ç½®
    if len(config) >= MAX_CONFIG_LENGTH:
        return True
    # è¿‡æ»¤åŒé‡URLç¼–ç ï¼ˆ%2525ï¼‰çš„é…ç½®
    if '%2525' in config:
        return True
    return False

async def fetch_url(session, url):
    retries = 3
    for attempt in range(retries):
        try:
            async with session.get(url, timeout=REQUEST_TIMEOUT) as response:
                response.raise_for_status()
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                text_content = ""
                # å°è¯•ä»å„ç§HTMLå…ƒç´ ä¸­æå–æ–‡æœ¬
                for element in soup.find_all(['pre', 'code', 'p', 'div', 'li', 'span', 'td']):
                    text_content += element.get_text(separator='\n', strip=True) + "\n"
                if not text_content:
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šå…ƒç´ ï¼Œä½¿ç”¨æ•´ä¸ªé¡µé¢æ–‡æœ¬
                    text_content = soup.get_text(separator=' ', strip=True)
                logging.info(f"Successfully fetched: {url} (attempt {attempt+1}/{retries})")
                return url, text_content
        except Exception as e:
            if attempt < retries - 1:
                logging.warning(f"Failed to fetch or process {url} (attempt {attempt+1}/{retries}): {e}. Retrying...")
                await asyncio.sleep(1)  # é‡è¯•å‰ç­‰å¾…1ç§’
            else:
                logging.error(f"Failed to fetch or process {url} after {retries} attempts: {e}")
                return url, None

def find_matches(text, categories_data):
    matches = {category: set() for category in categories_data}
    for category, patterns in categories_data.items():
        for pattern_str in patterns:
            if not isinstance(pattern_str, str):
                continue
            try:
                is_protocol_pattern = any(proto_prefix in pattern_str for proto_prefix in [p.lower() + "://" for p in PROTOCOL_CATEGORIES])
                if category in PROTOCOL_CATEGORIES or is_protocol_pattern:
                    pattern = re.compile(pattern_str, re.IGNORECASE | re.MULTILINE)
                    found = pattern.findall(text)
                    if found:
                        cleaned_found = {item.strip() for item in found if item.strip()}
                        matches[category].update(cleaned_found)
            except re.error as e:
                logging.error(f"Regex error for '{pattern_str}' in category '{category}': {e}")
    return {k: v for k, v in matches.items() if v}

def save_to_file(directory, category_name, items_set):
    if not items_set:
        return False, 0
    file_path = os.path.join(directory, f"{category_name}.txt")
    count = len(items_set)
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            for item in sorted(list(items_set)):
                f.write(f"{item}\n")
        logging.info(f"Saved {count} items to {file_path}")
        return True, count
    except Exception as e:
        logging.error(f"Failed to write file {file_path}: {e}")
        return False, 0

def generate_simple_readme(protocol_counts, country_counts, all_keywords_data, github_repo_path="areyrteuurt/Auto-Tomas", github_branch="main"):
    tz = pytz.timezone('Asia/Shanghai')
    now = datetime.now(tz)
    time_str = now.strftime("%H:%M")
    date_str = now.strftime("%Y-%m-%d")
    timestamp = f"æœ€åæ›´æ–°: {time_str} {date_str}"

    raw_github_base_url_protocols = f"https://raw.githubusercontent.com/{github_repo_path}/refs/heads/{github_branch}/configs/protocols"
    raw_github_base_url_countries = f"https://raw.githubusercontent.com/{github_repo_path}/refs/heads/{github_branch}/configs/countries"

    total_configs = sum(protocol_counts.values())

    md_content = f"""# ğŸš€ Auto-Tomas

<p align="center">
  <img src="https://img.shields.io/github/license/{github_repo_path}?style=flat-square&color=blue" alt="License" />
  <img src="https://img.shields.io/badge/python-3.9%2B-3776AB?style=flat-square&logo=python" alt="Python 3.9+" />
  <img src="https://img.shields.io/github/actions/workflow/status/{github_repo_path}/scraper.yml?style=flat-square" alt="GitHub Workflow Status" />
  <img src="https://img.shields.io/github/last-commit/{github_repo_path}?style=flat-square" alt="Last Commit" />
  <br>
  <img src="https://img.shields.io/github/issues/{github_repo_path}?style=flat-square" alt="GitHub Issues" />
  <img src="https://img.shields.io/badge/Configs-{total_configs}-blue?style=flat-square" alt="Total Configs" />
  <img src="https://img.shields.io/github/stars/{github_repo_path}?style=social" alt="GitHub Stars" />
  <img src="https://img.shields.io/badge/status-active-brightgreen?style=flat-square" alt="Project Status" />
  <img src="https://img.shields.io/badge/language-ä¸­æ–‡%20%26%20English-007EC6?style=flat-square" alt="Language" />
</p>

## {timestamp}

---

## ğŸ“– å…³äºé¡¹ç›®
è¿™ä¸ªé¡¹ç›®è‡ªåŠ¨ä»å„ç§æ¥æºæ”¶é›†å’Œåˆ†ç±»VPNé…ç½®ï¼ˆå¦‚V2Rayã€Trojanå’ŒShadowsocksç­‰ä¸åŒåè®®ï¼‰ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä¸ºç”¨æˆ·æä¾›æœ€æ–°ä¸”å¯é çš„é…ç½®ã€‚



---

## ğŸ“ åè®®é…ç½®
{f'ç›®å‰æœ‰ {total_configs} ä¸ªé…ç½®å¯ç”¨ã€‚' if total_configs else 'æœªæ‰¾åˆ°ä»»ä½•åè®®é…ç½®ã€‚'}

<div align="center">

| åè®® | æ•°é‡ | ä¸‹è½½é“¾æ¥ |
|:-------:|:-----:|:------------:|
"""
    # æ·»åŠ æ±‡æ€»è¡Œï¼Œæ˜¾ç¤ºAllProtocols.txtçš„ä¿¡æ¯
    all_protocols_link = f"{raw_github_base_url_protocols}/AllProtocols.txt"
    md_content += f"| **æ±‡æ€»** | **{total_configs}** | [`AllProtocols.txt`]({all_protocols_link}) |\n"
    
    if protocol_counts:
        for category_name, count in sorted(protocol_counts.items()):
            file_link = f"{raw_github_base_url_protocols}/{category_name}.txt"
            md_content += f"| {category_name} | {count} | [`{category_name}.txt`]({file_link}) |\n"
    else:
        md_content += "| - | - | - |\n"

    md_content += "</div>\n\n---\n\n"

    md_content += f"""
## ğŸŒ å›½å®¶é…ç½®
{f'é…ç½®å·²æŒ‰å›½å®¶åç§°åˆ†ç±»ã€‚' if country_counts else 'æœªæ‰¾åˆ°ä»»ä½•å›½å®¶ç›¸å…³é…ç½®ã€‚'}

<div align="center">

| å›½å®¶ | æ•°é‡ | ä¸‹è½½é“¾æ¥ |
|:----:|:-----:|:------------:|
"""
    if country_counts:
        for country_category_name, count in sorted(country_counts.items()):
            flag_image_markdown = ""
            chinese_name_str = ""
            iso_code_original_case = ""

            if country_category_name in all_keywords_data:
                keywords_list = all_keywords_data[country_category_name]
                if keywords_list and isinstance(keywords_list, list):
                    iso_code_lowercase_for_url = ""
                    for item in keywords_list:
                        if isinstance(item, str) and len(item) == 2 and item.isupper() and item.isalpha():
                            iso_code_lowercase_for_url = item.lower()
                            iso_code_original_case = item
                            break
                    if iso_code_lowercase_for_url:
                        flag_image_url = f"https://flagcdn.com/w20/{iso_code_lowercase_for_url}.png"
                        flag_image_markdown = f'<img src="{flag_image_url}" width="20" alt="{country_category_name} flag"> '
                    for item in keywords_list:
                        if isinstance(item, str):
                            if iso_code_original_case and item == iso_code_original_case:
                                continue
                            if item.lower() == country_category_name.lower() and not is_persian_like(item):
                                continue
                            if len(item) in [2, 3] and item.isupper() and item.isalpha() and item != iso_code_original_case:
                                continue
                            # æ£€æŸ¥æ˜¯å¦ä¸ºä¸­æ–‡åç§°
                            if any('\u4e00' <= c <= '\u9fff' for c in item):
                                chinese_name_str = item
                                break
            display_parts = []
            if flag_image_markdown:
                display_parts.append(flag_image_markdown)
            display_parts.append(country_category_name)
            if chinese_name_str:
                display_parts.append(f"({chinese_name_str})")
            country_display_text = " ".join(display_parts)
            file_link = f"{raw_github_base_url_countries}/{country_category_name}.txt"
            md_content += f"| {country_display_text} | {count} | [`{country_category_name}.txt`]({file_link}) |\n"
    else:
        md_content += "| - | - | - |\n"

    md_content += "</div>\n\n---\n\n"

    md_content += """
## ğŸ› ï¸ ä½¿ç”¨æ–¹æ³•
1. **ä¸‹è½½é…ç½®**: ä»ä¸Šæ–¹è¡¨æ ¼ä¸­ï¼Œä¸‹è½½æ‚¨éœ€è¦çš„æ–‡ä»¶ï¼ˆæ ¹æ®åè®®æˆ–å›½å®¶ï¼‰ã€‚
2. **æ¨èå®¢æˆ·ç«¯**:
   - **V2Ray**: [v2rayNG](https://github.com/2dust/v2rayNG) (å®‰å“)ï¼Œ[Hiddify](https://github.com/hiddify/hiddify-app/releases) (Mac)ï¼Œ[V2RayN](https://github.com/2dust/v2rayN/releases) (Windows)
   - **NekoRey_pro**: [NekoRey](https://github.com/Mahdi-zarei/nekoray/releases) (Mac)ï¼Œ[Karing](https://github.com/KaringX/karing/releases)
   - **sing-box**: [Sing-Box](https://github.com/SagerNet/sing-box/releases)
3. å°†é…ç½®æ–‡ä»¶å¯¼å…¥æ‚¨çš„å®¢æˆ·ç«¯å¹¶æµ‹è¯•è¿æ¥ã€‚

> **å»ºè®®**: ä¸ºè·å¾—æœ€ä½³æ€§èƒ½ï¼Œè¯·å®šæœŸæ£€æŸ¥å’Œæ›´æ–°é…ç½®ã€‚

---
## ğŸ“¢ æ³¨æ„äº‹é¡¹
- æœ¬é¡¹ç›®ä»…ç”¨äºå­¦ä¹ å’Œç ”ç©¶ç›®çš„ã€‚
- è¯·æ ¹æ®æ‚¨æ‰€åœ¨å›½å®¶çš„æ³•å¾‹è´Ÿè´£ä»»åœ°ä½¿ç”¨é…ç½®ã€‚
- å¦‚é‡é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·ä½¿ç”¨ [Issues](https://github.com/Eleven1985/Scrape-By-Country/issues) éƒ¨åˆ†ã€‚
"""


    try:
        with open(README_FILE, 'w', encoding='utf-8') as f:
            f.write(md_content)
        logging.info(f"Successfully generated {README_FILE}")
    except Exception as e:
        logging.error(f"Failed to write {README_FILE}: {e}")

async def main():
    if not os.path.exists(URLS_FILE) or not os.path.exists(KEYWORDS_FILE):
        logging.critical("Input files not found.")
        return

    with open(URLS_FILE, 'r', encoding='utf-8') as f:
        urls = [line.strip() for line in f if line.strip()]
    with open(KEYWORDS_FILE, 'r', encoding='utf-8') as f:
        categories_data = json.load(f)

    protocol_patterns_for_matching = {
        cat: patterns for cat, patterns in categories_data.items() if cat in PROTOCOL_CATEGORIES
    }
    country_keywords_for_naming = {
        cat: patterns for cat, patterns in categories_data.items() if cat not in PROTOCOL_CATEGORIES
    }
    country_category_names = list(country_keywords_for_naming.keys())

    logging.info(f"Loaded {len(urls)} URLs and "
                 f"{len(categories_data)} total categories from key.json.")

    tasks = []
    sem = asyncio.Semaphore(CONCURRENT_REQUESTS)
    async def fetch_with_sem(session, url_to_fetch):
        async with sem:
            return await fetch_url(session, url_to_fetch)
    async with aiohttp.ClientSession() as session:
        fetched_pages = await asyncio.gather(*[fetch_with_sem(session, u) for u in urls])

    final_configs_by_country = {cat: set() for cat in country_category_names}
    final_all_protocols = {cat: set() for cat in PROTOCOL_CATEGORIES}

    logging.info("Processing pages for config name association...")
    for url, text in fetched_pages:
        if not text:
            continue

        page_protocol_matches = find_matches(text, protocol_patterns_for_matching)
        all_page_configs_after_filter = set()
        for protocol_cat_name, configs_found in page_protocol_matches.items():
            if protocol_cat_name in PROTOCOL_CATEGORIES:
                for config in configs_found:
                    if should_filter_config(config):
                        continue
                    all_page_configs_after_filter.add(config)
                    final_all_protocols[protocol_cat_name].add(config)

        for config in all_page_configs_after_filter:
            name_to_check = None
            if '#' in config:
                try:
                    potential_name = config.split('#', 1)[1]
                    name_to_check = unquote(potential_name).strip()
                    if not name_to_check:
                        name_to_check = None
                except IndexError:
                    pass

            if not name_to_check:
                if config.startswith('ssr://'):
                    name_to_check = get_ssr_name(config)
                elif config.startswith('vmess://'):
                    name_to_check = get_vmess_name(config)
                elif config.startswith('vless://'):
                    name_to_check = get_vless_name(config)
                elif config.startswith('trojan://'):
                    name_to_check = get_trojan_name(config)
                elif config.startswith('ss://'):
                    name_to_check = get_shadowsocks_name(config)
                elif config.startswith('hy2://'):
                    name_to_check = get_hysteria2_name(config)
                elif config.startswith('wg://') or 'WireGuard' in config:
                    name_to_check = get_wireguard_name(config)

            if not name_to_check:
                continue

            current_name_to_check_str = name_to_check if isinstance(name_to_check, str) else ""

            for country_name_key, keywords_for_country_list in country_keywords_for_naming.items():
                if not isinstance(keywords_for_country_list, list):
                    continue
                    
                # æå–æ‰€æœ‰æœ‰æ•ˆçš„å›½å®¶å…³é”®å­—ï¼ˆæ’é™¤emojiä½†ä¿ç•™å›½å®¶ä»£ç å’Œåç§°ï¼‰
                text_keywords_for_country = []
                for kw in keywords_for_country_list:
                    if not isinstance(kw, str):
                        continue
                        
                    # ä¿ç•™å›½å®¶ä»£ç ï¼ˆ2-3ä¸ªå¤§å†™å­—æ¯ï¼‰
                    if 2 <= len(kw) <= 3 and kw.isupper() and kw.isalpha():
                        text_keywords_for_country.append(kw)
                        continue
                        
                    # ä¿ç•™å®Œæ•´çš„å›½å®¶åç§°ï¼ˆéemojiï¼‰
                    if not (1 <= len(kw) <= 7 and not kw.isalnum()):  # æ’é™¤å¯èƒ½æ˜¯emojiçš„å­—ç¬¦ä¸²
                        # å³ä½¿æ˜¯æ³¢æ–¯è¯­ï¼Œå¦‚æœä¸å›½å®¶åç§°åŒ¹é…ä¹Ÿä¿ç•™
                        if not is_persian_like(kw) or kw.lower() == country_name_key.lower():
                            text_keywords_for_country.append(kw)
                
                # æ£€æŸ¥é…ç½®åç§°æ˜¯å¦åŒ…å«å›½å®¶å…³é”®å­—
                for keyword in text_keywords_for_country:
                    if not isinstance(keyword, str):
                        continue
                        
                    # å¯¹äºå›½å®¶ä»£ç ï¼Œä½¿ç”¨å•è¯è¾¹ç•ŒåŒ¹é…
                    if 2 <= len(keyword) <= 3 and keyword.isupper() and keyword.isalpha():
                        pattern = r'\b' + re.escape(keyword) + r'\b'
                        if re.search(pattern, current_name_to_check_str, re.IGNORECASE):
                            final_configs_by_country[country_name_key].add(config)
                            break
                    # å¯¹äºå›½å®¶åç§°ï¼Œä½¿ç”¨åŒ…å«åŒ¹é…
                    elif keyword.lower() in current_name_to_check_str.lower():
                        final_configs_by_country[country_name_key].add(config)
                        break
                else:
                    continue  # æœªæ‰¾åˆ°åŒ¹é…ï¼Œç»§ç»­ä¸‹ä¸€ä¸ªå›½å®¶
                break  # æ‰¾åˆ°åŒ¹é…ï¼Œè·³å‡ºå¾ªç¯

    if os.path.exists('configs'):
        shutil.rmtree('configs')
    os.makedirs(PROTOCOL_OUTPUT_DIR, exist_ok=True)
    os.makedirs(COUNTRY_OUTPUT_DIR, exist_ok=True)
    logging.info(f"Saving protocol files to directory: {PROTOCOL_OUTPUT_DIR}")
    logging.info(f"Saving country files to directory: {COUNTRY_OUTPUT_DIR}")

    protocol_counts = {}
    country_counts = {}

    for category, items in final_all_protocols.items():
        saved, count = save_to_file(PROTOCOL_OUTPUT_DIR, category, items)
        if saved:
            protocol_counts[category] = count
    
    # Merge all protocols into one file (ä¿æŒä¸å…¶ä»–åè®®æ–‡ä»¶å‘½åä¸€è‡´)
    all_protocols_combined = set()
    for category, items in final_all_protocols.items():
        all_protocols_combined.update(items)
    save_to_file(PROTOCOL_OUTPUT_DIR, "AllProtocols", all_protocols_combined)
    
    for category, items in final_configs_by_country.items():
        saved, count = save_to_file(COUNTRY_OUTPUT_DIR, category, items)
        if saved:
            country_counts[category] = count

    generate_simple_readme(protocol_counts, country_counts, categories_data,
                          github_repo_path="areyrteuurt/Auto-Tomas",
                          github_branch="main")

    logging.info("--- Script Finished ---")

if __name__ == "__main__":
    asyncio.run(main())
